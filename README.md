It appears that Bing chatbot (Copilot) was hacked using prompt injection. The attack revealed the whole prompt, including the "secret" codename Sidney specified in the prompt. The prompt contained rules, style, syntax, and user context for the bot. The only preventative measure was one statement. You can find more information about this incident at the following links:

- [Parappa_the_lagger.md](https://github.com/mshojaei77/Bing-initial-prompt/blob/main/Parappa_the_lagger.md)
- [Kevin Liu.md](https://github.com/mshojaei77/Bing-initial-prompt/blob/main/Kevin%20Liu.md)
